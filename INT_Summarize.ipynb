{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl5rNMI3yhw2PPGFhviNEx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osullik/bc-autoreporter/blob/main/INT_Summarize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "-If8c_sOCKoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiYc1tsC_wYi",
        "outputId": "4942c358-5387-4843-eaea-4c418a871a07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg_5OL4q-56M",
        "outputId": "c6881db3-fcd4-4bc2-d9e2-ab235e8db0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   print(\"Using GPU\")\n",
        "   device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3Wojs_cVXZd",
        "outputId": "cb0400bc-e2c7-4c72-c147-07e8b4f607cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in data\n",
        "Assume each set of evaluations corresponds to the same employee. Different employees will be read in separately and run through the model separately to generate per-employee summaries."
      ],
      "metadata": {
        "id": "zqWP-HHWCN8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_text = [{'entity': 'HomerSimpson', 'date': '2021-10-14', 'tagList': ['resilience', 'problem-solving', 'safetyconsciousness'], 'observation': 'On October 14th, 2021, @HomerSimpson was observed to perform to an excellent standard. This was evidenced by his quick response to a power outage in Sector 7G, restoring power and ensuring that critical systems remained operational. His actions show #resilience #problem-solving #safetyconsciousness.\\n', 'sentiment': 69.08, 'adjectives': ['excellent', 'quick', 'critical', 'resilience', 'problem-solving'], 'observer': 'MontgomeryBurns', 'lastModified': '2023-04-09T01:16:32'}, {'entity': 'HomerSimpson', 'date': '2021-08-25', 'tagList': ['forgetfulness', 'lackofattention'], 'observation': 'On August 25th, @HomerSimpson was observed to perform to a poor standard. This was evidenced by his forgetting to inform Mr. Burns of an important meeting, resulting in a missed opportunity for the company. His actions show #forgetfulness and #lackofattention.\\n', 'sentiment': -17.79, 'adjectives': ['poor', 'important', 'missed', 'forgetfulness', 'lackofattention'], 'observer': 'MontgomeryBurns', 'lastModified': '2023-04-09T01:16:32'}, {'entity': 'HomerSimpson', 'date': '2021-11-29', 'tagList': ['Conflict', 'Tension', 'TragicAccident'], 'observation': \"On November 29th, @FrankGrimes was observed to still be at odds with @HomerSimpson. Despite attempts by other employees to mediate the situation, the two were unable to resolve their differences and tensions continued to escalate. This culminated in the tragic accident that took Frank's life later that day. #Conflict #Tension #TragicAccident\\n\", 'sentiment': -73.50999999999999, 'adjectives': ['still', 'other', 'unable', 'tragic', 'later', 'Conflict'], 'observer': 'MontgomeryBurns', 'lastModified': '2023-04-09T01:16:34'}]"
      ],
      "metadata": {
        "id": "TVAkzSdUn5kL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Functions"
      ],
      "metadata": {
        "id": "5FLwC_4ZCZB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate chunks of text \\ sentences <= 512 tokens\n",
        "def nest_sentences(document):\n",
        "  nested = []\n",
        "  sent = [\"summarize\"]  # need to provide task instruction as the first token\n",
        "  length = 1\n",
        "  for sentence in nltk.sent_tokenize(document):\n",
        "    length += len(sentence)\n",
        "    if length < 512:\n",
        "      sent.append(sentence)\n",
        "    else:\n",
        "      nested.append(sent)\n",
        "      sent = [\"summarize\"]  # need to provide task instruction as the first token\n",
        "      length = 1\n",
        "\n",
        "  if len(sent) > 1:\n",
        "    nested.append(sent)\n",
        "\n",
        "  return nested"
      ],
      "metadata": {
        "id": "vi2qDgFK0MWt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization function\n",
        "Nested function feeds input sentence by sentence, which doesn't allow summarization across a report. Use generate_summary instead."
      ],
      "metadata": {
        "id": "cAqGHVrU1Qzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate summary on text with <= 512 tokens\n",
        "def generate_summary(sentences, tokenizer, model):\n",
        "  input_tokenized = tokenizer.encode(sentences, truncation=True, return_tensors='pt')\n",
        "  input_tokenized = input_tokenized.to(device)\n",
        "  summary_ids = model.to(device).generate(input_tokenized,\n",
        "                                          num_beams=4,\n",
        "                                          min_length=50,\n",
        "                                          max_length=1000,\n",
        "                                          early_stopping=True)\n",
        "  output = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
        "  return output"
      ],
      "metadata": {
        "id": "ky9F76QSns8Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Tokenizer\n",
        "Corresponding T5 tokenizer from Hugging Face library"
      ],
      "metadata": {
        "id": "uCCWc1NN1T-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model\n",
        "Pretrained T5 model from Hugging Face library. This is a generic model that can do translation, summarization, etc. We will pass it the 'summarize' command appended to the front of the input stream to trigger a summary output from the model.\n",
        "\n",
        "https://towardsdatascience.com/simple-abstractive-text-summarization-with-pretrained-t5-text-to-text-transfer-transformer-10f6d602c426 "
      ],
      "metadata": {
        "id": "7IuTZPbsCUE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize\n",
        "Generate summary text given preprocessed input text containing a set of reports for a given employee."
      ],
      "metadata": {
        "id": "rPA_hObYBvaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def employee_summarize(list_of_dicts):\n",
        "  employee_record = ''\n",
        "  for _dict in list_of_dicts:\n",
        "    employee_record = employee_record + _dict['observation'].replace(\"\\n\",\" \")\n",
        "    \n",
        "  tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "  model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "  nested_text = nest_sentences(employee_record)\n",
        "\n",
        "  employee_summary = []\n",
        "  for i in range(len(nested_text)):\n",
        "    inputs = []\n",
        "\n",
        "    for j in nested_text[i]:\n",
        "      inputs.append(j.split())\n",
        "\n",
        "    inputs_squeezed = np.concatenate(inputs, axis=0 )\n",
        "    inputs_squeezed = [j + ' ' for j in inputs_squeezed]\n",
        "    employee_summary.append(generate_summary(' '.join(inputs_squeezed), tokenizer, model))\n",
        "\n",
        "  sub_summaries = [''.join(ele) for ele in employee_summary]\n",
        "  return ''.join(sub_summaries)"
      ],
      "metadata": {
        "id": "p1V8reZsoI5a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_summarize(original_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "uv0jrqUFoVwQ",
        "outputId": "dde122de-8a4d-4c5a-9890-673287c97a82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@HomerSimpson was observed to perform to an excellent standard . this was evidenced by his quick response to a power outage in Sector 7G . his actions show #resilience #problem-solving #safetyconsciousness .@FrankGrimes was observed to still be at odds with @HomerSimpson . the two were unable to resolve their differences and tensions continued to escalate . this culminated in the tragic accident that took Frank's life later that day .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}